{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdDKKj1ZnKtOMb/ipV15ch",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fudulism/ScrappedData/blob/main/steamscrap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLPrD6YbSB1v"
      },
      "outputs": [],
      "source": [
        "!pip install requests beautifulsoup4 --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Beautiful Soup\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Import csv module\n",
        "import csv\n",
        "import json\n",
        "\n",
        "# Import regex\n",
        "import re"
      ],
      "metadata": {
        "id": "iJzjH_2ASXtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the webpage using requests\n",
        "\n",
        "# URL of the website to be scraped for the current search query\n",
        "url = 'https://store.steampowered.com/search/?filter=topsellers'\n",
        "\n",
        "# Send a GET request to the specified URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Get the content of the downloaded page and save in a variable\n",
        "page_content = response.text\n",
        "page_content"
      ],
      "metadata": {
        "id": "aE6D2hs8SX9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert the file to a beautiful soup file\n",
        "doc = BeautifulSoup(page_content, 'html.parser')\n",
        "\n",
        "# Find all the games on the page\n",
        "games = doc.find_all('div', {'class': 'responsive_search_name_combined'})\n",
        "\n",
        "# Create the scraper component to save the result as a CSV file using the CSV module\n",
        "with open('games_topsellers.json', mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = json.writer(file)\n",
        "    writer.writerow(['Name', 'Published Date', 'Original Price', 'Discount Price', 'Reviews'])\n",
        "\n",
        "    # Loop through each game and extract the relevant information\n",
        "    for game in games:\n",
        "        name = game.find('span', {'class': 'title'}).text\n",
        "        published_date = game.find('div', {'class': 'col search_released responsive_secondrow'}).text.strip()\n",
        "\n",
        "        # Check if the element is present before accessing the text attribute\n",
        "        original_price_elem = game.find('div', {'class': 'discount_original_price'})\n",
        "        original_price = original_price_elem.text.strip() if original_price_elem else 'N/A'\n",
        "\n",
        "        discount_price_elem = game.find('div', {'class': 'discount_final_price'})\n",
        "        discount_price = discount_price_elem.text.strip() if discount_price_elem else 'N/A'\n",
        "\n",
        "        # Extract review information using regular expressions\n",
        "        review_summary = game.find('span', {'class': 'search_review_summary'})\n",
        "        reviews_html = review_summary['data-tooltip-html'] if review_summary else 'N/A'\n",
        "\n",
        "        # Use regular expressions to extract the number of reviews\n",
        "        match = re.search(r'(\\d+,*\\d*)\\s+user reviews', reviews_html)\n",
        "        reviews_number = match.group(1).replace(',', '') if match else 'N/A'\n",
        "\n",
        "        # Write the extracted information to the CSV file\n",
        "        writer.writerow([name, published_date, original_price, discount_price, reviews_number])\n"
      ],
      "metadata": {
        "id": "JggOkWh-SYwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of search filters\n",
        "search_filters = ['topsellers', 'mostplayed', 'newreleases', 'upcomingreleases']\n",
        "\n",
        "# Create a CSV file to store the scraped data\n",
        "with open('games_all.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Name', 'Published_Date', 'Original Price', 'Discount Price', 'Reviews', 'Search Query'])\n",
        "\n",
        "    # Loop through each search query\n",
        "    for filter in search_filters:\n",
        "        # URL of the website to be scraped for the current search query\n",
        "        url = f'https://store.steampowered.com/search/?filter={filter}'\n",
        "\n",
        "        # Send a GET request to the specified URL\n",
        "        response = requests.get(url)\n",
        "\n",
        "        # Parse the HTML content of the page using BeautifulSoup\n",
        "        webpage = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Find the total number of pages\n",
        "        total_pages = int(webpage.find('div', {'class': 'search_pagination_right'}).find_all('a')[-2].text)\n",
        "\n",
        "        # Counter to keep track of the number of lines written\n",
        "        line_count = 0\n",
        "\n",
        "        # Loop through each page and extract the relevant information\n",
        "        for page in range(1, total_pages + 1):\n",
        "            # Send a GET request to the specified URL\n",
        "            response = requests.get(url + '&page=' + str(page))\n",
        "\n",
        "            # Parse the HTML content of the page using BeautifulSoup\n",
        "            doc = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Find all the games on the page\n",
        "            games = doc.find_all('div', {'class': 'responsive_search_name_combined'})\n",
        "\n",
        "            # Loop through each game and extract the relevant information\n",
        "            for game in games:\n",
        "                name = game.find('span', {'class': 'title'}).text\n",
        "                published_date = game.find('div', {'class': 'col search_released responsive_secondrow'}).text.strip()\n",
        "\n",
        "                # Check if the element is present before accessing the text attribute\n",
        "                original_price_elem = game.find('div', {'class': 'discount_original_price'})\n",
        "                original_price = original_price_elem.text.strip() if original_price_elem else 'N/A'\n",
        "\n",
        "                discount_price_elem = game.find('div', {'class': 'discount_final_price'})\n",
        "                discount_price = discount_price_elem.text.strip() if discount_price_elem else 'N/A'\n",
        "\n",
        "                # Extract review information using regular expressions\n",
        "                review_summary = game.find('span', {'class': 'search_review_summary'})\n",
        "                reviews_html = review_summary['data-tooltip-html'] if review_summary else 'N/A'\n",
        "\n",
        "                # Use regular expressions to extract the number of reviews\n",
        "                match = re.search(r'(\\d+,*\\d*)\\s+user reviews', reviews_html)\n",
        "                reviews_number = match.group(1).replace(',', '') if match else 'N/A'\n",
        "\n",
        "                # Write the extracted information to the CSV file\n",
        "                writer.writerow([name, published_date, original_price, discount_price, reviews_number, filter])\n",
        "\n",
        "                # Increment the line count\n",
        "                line_count += 1\n",
        "\n",
        "                # Stop scraping if we have reached the minimum data requirement\n",
        "                if line_count > 100:\n",
        "                    break\n",
        "\n",
        "            # Stop scraping if we have reached the minimum data requirement\n",
        "            if line_count > 100:\n",
        "                break"
      ],
      "metadata": {
        "id": "M2qTW6OaSYsl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function that takes url and get the total page\n",
        "def get_total_pages(url):\n",
        "    response = requests.get(url)\n",
        "    doc = BeautifulSoup(response.content, 'html.parser')\n",
        "    total_pages = int(doc.find('div', {'class': 'search_pagination_right'}).find_all('a')[-2].text)\n",
        "    return total_pages\n",
        "\n",
        "# Create a function that extracts game info from the webpage\n",
        "def extract_game_info(game):\n",
        "    name = game.find('span', {'class': 'title'}).text\n",
        "    published_date = game.find('div', {'class': 'col search_released responsive_secondrow'}).text.strip()\n",
        "\n",
        "    original_price_elem = game.find('div', {'class': 'discount_original_price'})\n",
        "    original_price = original_price_elem.text.strip() if original_price_elem else 'N/A'\n",
        "\n",
        "    discount_price_elem = game.find('div', {'class': 'discount_final_price'})\n",
        "    discount_price = discount_price_elem.text.strip() if discount_price_elem else 'N/A'\n",
        "\n",
        "    review_summary = game.find('span', {'class': 'search_review_summary'})\n",
        "    reviews_html = review_summary['data-tooltip-html'] if review_summary else 'N/A'\n",
        "\n",
        "    match = re.search(r'(\\d+,*\\d*)\\s+user reviews', reviews_html)\n",
        "    reviews_number = match.group(1).replace(',', '') if match else 'N/A'\n",
        "\n",
        "    return name, published_date, original_price, discount_price, reviews_number\n",
        "\n",
        "# Create a function that scrapes the webpage\n",
        "def scrape_page(url, filter, writer):\n",
        "    # Invoking get total page function\n",
        "    total_pages = get_total_pages(url)\n",
        "\n",
        "    line_count = 0\n",
        "\n",
        "    for page in range(1, total_pages + 1):\n",
        "        response = requests.get(f\"{url}&page={page}\")\n",
        "        doc = BeautifulSoup(response.content, 'html.parser')\n",
        "        games = doc.find_all('div', {'class': 'responsive_search_name_combined'})\n",
        "\n",
        "        for game in games:\n",
        "            # Invoking the extract game info function\n",
        "            game_info = extract_game_info(game)\n",
        "            writer.writerow([*game_info, filter])\n",
        "\n",
        "            line_count += 1\n",
        "            if line_count > 100:\n",
        "                break\n",
        "\n",
        "        if line_count > 100:\n",
        "            break\n",
        "\n",
        "\n",
        "# Creating the main function that takes the scrape page function and do the actual scraping\n",
        "def main(search_queries=[\"topsellers\"]):\n",
        "\n",
        "    with open('games_all.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Name', 'Date', 'Original Price', 'Discount Price', 'Reviews', 'Search Filter'])\n",
        "\n",
        "        for filter in search_filters:\n",
        "            url = f'https://store.steampowered.com/search/?filter={filter}'\n",
        "            # Invoking the scrape page function\n",
        "            scrape_page(url, filter, writer)\n",
        "\n",
        "\n",
        "# Invoking the main function\n",
        "search_queries = ['topsellers', 'mostplayed', 'newreleases', 'upcomingreleases']\n",
        "main(search_queries)"
      ],
      "metadata": {
        "id": "s4EFxI2OSYlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: convert games_topsellers.csv to json\n",
        "\n",
        "import csv\n",
        "import json\n",
        "\n",
        "def csv_to_json(csv_file_path, json_file_path):\n",
        "  \"\"\"Converts a CSV file to a JSON file.\n",
        "\n",
        "  Args:\n",
        "    csv_file_path: The path to the CSV file.\n",
        "    json_file_path: The path to the JSON file to be created.\n",
        "  \"\"\"\n",
        "  data = []\n",
        "  with open(csv_file_path, 'r', encoding='utf-8') as csv_file:\n",
        "    csv_reader = csv.DictReader(csv_file)\n",
        "    for row in csv_reader:\n",
        "      data.append(row)\n",
        "\n",
        "  with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
        "    json.dump(data, json_file, indent=4)\n",
        "\n",
        "# Example usage:\n",
        "csv_to_json('games_topsellers.csv', 'games_topsellers.json')\n",
        "csv_to_json('games_all.csv', 'games_all.json')\n"
      ],
      "metadata": {
        "id": "HUCRLpvXSYR9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OCGQXvULSYJg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}